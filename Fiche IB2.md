pvalue > seuil : on accepte H0
pvalue < seuil : on rejette H0 (et donc on accepte H1)
- variables quantitatives (par exemple : la taille, le poids, ...)
- variables qualitatives (par exemple : la couleur des yeux, la
saison,...), avec un nombre fini de cat√©gories (les modalit√©s) qui
peuvent √™tre ordonn√©es ou non

Deux (principaux) tests du X¬≤ : QUALITATIF(S)
- Test du X2 d'ad√©quation (1 variable)
- Test du X2 d'homog√©n√©it√© (2 variables)

Une table de contingence contient le nombre d'occurences de chaque
modalit√© d'une variable qualitative, ou de chaque couple de modalit√©s si on a 2 variables.
> table(variable1)
> table(variable1, variable2)

X2 ad√©quation : 
> tab = table(sample)
> chisq.test(tab_sample,p=c(p1,p2,p3)) (ordre alphab√©tique)
H0 = la r√©partition des observations dans les di√©rents groupes est
conforme √† la r√©partition attendue

X2 homog√©n√©it√©/ind√©
> tab = table(samp1,samp2), puis chisq.test(tab)
H0 = les crit√®res qualitatifs consid√©r√©s sont ind√©pendants

Shapiro = normal
Fisher = m√™me variance
![[Pasted image 20230419120945.png]]

Corr√©lation : facteurs confusion A-> B et A -> C corr entre B et C
facteurs interm√©diaires : A -> B -> C corr√©lation A et C
hasard ; manip donn√©es

Tester normalit√© into test corr√©lation:
> cor.test(s1,s2, method = "pearson") normal
> cor.test(s1,s2, method = "spearman") pas normal
H0 = les √©chantillons ne sont pas corr√©l√©s

mod√®le lin√©aires : repr√©sentation simplifi√©e de la r√©alit√©
Une variable expliqu√©e (quantitative) et une/plusieurs variables explicatives quanti ou quali

mod = lm(y~x) ou lm(y~x1+x2)
test normalit√© des **r√©sidus** : shapiro.test(mod$residuals) 
summary(mod) et mod$coefficients pour avoir la r√©gression et les coeffs
a0 correspond √† la ligne (Intercept), a1 ligne x -> a0+a1x
plot(x,y) puis abline(mod)

si on veut comparer deux mod√®les il faut qu'ils soient emboit√©s et que leurs r√©sidus soient normaux

mod√®le sans facteur : > mod0 = lm(y ~ 1)
Comparaison mod√®les lin√©aires:
H0 : "les mod√®les sont √©quivalents en terme de pr√©diction"
- si on rejette l'hypoth√®se nulle, cela veut dire que l'un des mod√®les est meilleur pour expliquer les donn√©es : celui qui a le RSS le plus faible
- si on accepte l'hypoth√®se nulle, les mod√®les sont "similairement bons", on choisira donc le mod√®le le plus simple : celui qui a le moins de coefficients

> anova(mod1, mod2) 


si explicatif = qualitatif on pourra pr√©dire une valeur pour chaque modalit√© de la variable qualitative
dans les coeffs : la pr√©diction pour la modalit√© de r√©f√©rence (absent) correspond √† la ligne (Intercept)
la diff√©rence entre cette modalit√© et la deuxi√®me correspond √† la ligne suivante... etc etc tt est compar√© √† la modalit√© de ref.

Si la pvalue est inf√©rieure au seuil, le coefficient est significativement diff√©rent de 0 et il y a donc une diff√©rence par rapport √† la modalit√© de r√©f√©rence


Comparaison moyennes pour plus de 2 groupes.
boxplot(Taille ~ Antibiotique, data = donnees, main = "titre gn√©gn√©gn√©",xlab = "Concentration en antibiotique", ylab = "Taille des bact√©ries", col = rainbow(3))
- s√©parer tt les √©chantillons par modalit√©s dans des vecteurs
- v√©rifier leur normalit√© avec shapiro
- si tous normaux : bartlett.test(y~x)  H0=m√™me variance
- si normalit√© et homod√©asticit√© : > aov(y~x) ; sinon kruskal.test(y~x)  H0 = m√™me moyenne
mod= aov(y~x)
- si on trouve un effet et anova : TukeyHSD(mod), et faut voir pvalue inf √† alpha = diff signif. 


- si kruskal : test mann-whitney avec correction bonferroni (seuil alpha/nb comparaison) : wilcox.test(x1,y1). etc etc 
	si pval inf a alpha diff de moyenne
![[Pasted image 20230419123613.png]]

VE = variance expliqu√©e (entre moy groupe et moy globale)
K = nb variables?
![[Pasted image 20230419123819.png]]
VNE =  variance non-expliqu√©e (entre observations individuelles et moyenne du groupe)
![[Pasted image 20230419123828.png]]
Statistique de test de l'Anova : F =VE/VNE

prendre une variable en fonction d'une autre:
note_oui <- cm4$ Note.IB2[cm4 $ Master == "Oui"]






ACP :
On cherche √† r√©duire la dimension en projetant sur des axes principaux
(qui maximisent l'inertie projet√©e)
=> r√©duire la redondance et le nombre de variables = m√©thode de repr√©sentation!!!! pas test

charger ade4 : library(ade4)

pca.olympic = dudi.pca(olympic$tab,scale=T,scannf=F,nf=2)
cr√©e l'ACP

scatter(pca.olympic, posieig = "bottomright")
*scatter de tt les observations selon les 2 axes conserv√©s+ valeurs graphs en bas√† droite*

individus = lignes : pca.olympic$li